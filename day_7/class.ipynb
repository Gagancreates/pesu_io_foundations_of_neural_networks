{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pfXvFR1RabPI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight=np.array([133, 160, 152, 120])\n",
        "height=np.array([65, 72, 70, 60])\n"
      ],
      "metadata": {
        "id": "qBt1l7KybMVa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.array([0, 1, 1, 0]).reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "Iz_LAeWpbOXn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def minmax(x):\n",
        "  return (x-np.min(x))/(np.max(x)-np.min(x))"
      ],
      "metadata": {
        "id": "Z3umN6SBbRrU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight, height=minmax(weight), minmax(height)"
      ],
      "metadata": {
        "id": "q6ya9s2ZcylD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array([weight, height])\n",
        "x=x.T"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UBo2oXFYc-xY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hidden layer\n",
        "np.random.seed(42)\n",
        "w1=np.random.rand(2,2)\n",
        "b1=np.random.rand(1, 2)\n",
        "z1= np.dot(x, w1)+b1\n",
        "a1=sigmoid(z1)\n",
        "a1"
      ],
      "metadata": {
        "id": "iQm27N7MdDFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ouptput layer\n",
        "np.random.seed(42)\n",
        "w2=np.random.rand(2, 1)\n",
        "b2=np.random.rand()\n",
        "z2=np.dot(a1, w2)+b2\n",
        "\n",
        "y_pred=sigmoid(z2)"
      ],
      "metadata": {
        "id": "R9EqND8Dectx"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "def bce_loss(y_true, y_pred):\n",
        "  return -np.mean(y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))"
      ],
      "metadata": {
        "id": "7eIzMLUsfsgN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model\n",
        "np.random.seed(42)\n",
        "w1=np.random.rand(2,2)\n",
        "b1=np.random.rand(1, 2)\n",
        "w2=np.random.rand(2, 1)\n",
        "b2=np.random.rand()\n",
        "\n",
        "alpha=0.1\n",
        "epochs=5000\n",
        "m=x.shape[0]     #batch size\n",
        "\n",
        "for i in range(epochs):\n",
        "  #forward pass\n",
        "  z1=np.dot(x, w1.T)+b1\n",
        "  a1=sigmoid(z1)\n",
        "  z2=np.dot(a1, w2) + b2\n",
        "  y_pred=sigmoid(z2)\n",
        "\n",
        "  loss=bce_loss(y_true, y_pred)   #loss at each epoch\n",
        "\n",
        "  #backpropagation\n",
        "  dz2=y_pred-y_true\n",
        "  dw2=np.dot(a1.T, dz2)/m\n",
        "  db2=np.sum(dz2, axis=0)/m\n",
        "\n",
        "  dz1=np.dot(dz2, w2.T)*sigmoid_derivative(z1)\n",
        "  dw1=np.dot(dz1.T, x)/m\n",
        "  db1=np.sum(dz1, axis=0, keepdims=True)/m\n",
        "\n",
        "  #gradient descent/updating the weights\n",
        "  w2-=alpha*dw2\n",
        "  b2-=alpha*db2\n",
        "  w1-=alpha*dw1\n",
        "  b1-=alpha*db1\n",
        "\n",
        "  if(i%100==0):\n",
        "    print(f\"Loss at Epoch {i} is {loss:4f}\")\n",
        "\n",
        "#final set of weights and bias\n",
        "print(\"final predictions: \", y_pred)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx19grpPgP_I",
        "outputId": "d0faf053-e043-463b-fea7-d020b988057c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at Epoch 0 is 0.830593\n",
            "Loss at Epoch 100 is 0.654342\n",
            "Loss at Epoch 200 is 0.609321\n",
            "Loss at Epoch 300 is 0.546541\n",
            "Loss at Epoch 400 is 0.468799\n",
            "Loss at Epoch 500 is 0.388657\n",
            "Loss at Epoch 600 is 0.317453\n",
            "Loss at Epoch 700 is 0.259370\n",
            "Loss at Epoch 800 is 0.213798\n",
            "Loss at Epoch 900 is 0.178482\n",
            "Loss at Epoch 1000 is 0.151066\n",
            "Loss at Epoch 1100 is 0.129594\n",
            "Loss at Epoch 1200 is 0.112574\n",
            "Loss at Epoch 1300 is 0.098903\n",
            "Loss at Epoch 1400 is 0.087776\n",
            "Loss at Epoch 1500 is 0.078604\n",
            "Loss at Epoch 1600 is 0.070954\n",
            "Loss at Epoch 1700 is 0.064503\n",
            "Loss at Epoch 1800 is 0.059009\n",
            "Loss at Epoch 1900 is 0.054287\n",
            "Loss at Epoch 2000 is 0.050194\n",
            "Loss at Epoch 2100 is 0.046620\n",
            "Loss at Epoch 2200 is 0.043476\n",
            "Loss at Epoch 2300 is 0.040694\n",
            "Loss at Epoch 2400 is 0.038218\n",
            "Loss at Epoch 2500 is 0.036002\n",
            "Loss at Epoch 2600 is 0.034008\n",
            "Loss at Epoch 2700 is 0.032208\n",
            "Loss at Epoch 2800 is 0.030574\n",
            "Loss at Epoch 2900 is 0.029086\n",
            "Loss at Epoch 3000 is 0.027726\n",
            "Loss at Epoch 3100 is 0.026478\n",
            "Loss at Epoch 3200 is 0.025331\n",
            "Loss at Epoch 3300 is 0.024272\n",
            "Loss at Epoch 3400 is 0.023292\n",
            "Loss at Epoch 3500 is 0.022383\n",
            "Loss at Epoch 3600 is 0.021538\n",
            "Loss at Epoch 3700 is 0.020750\n",
            "Loss at Epoch 3800 is 0.020014\n",
            "Loss at Epoch 3900 is 0.019326\n",
            "Loss at Epoch 4000 is 0.018680\n",
            "Loss at Epoch 4100 is 0.018074\n",
            "Loss at Epoch 4200 is 0.017503\n",
            "Loss at Epoch 4300 is 0.016965\n",
            "Loss at Epoch 4400 is 0.016458\n",
            "Loss at Epoch 4500 is 0.015978\n",
            "Loss at Epoch 4600 is 0.015524\n",
            "Loss at Epoch 4700 is 0.015093\n",
            "Loss at Epoch 4800 is 0.014684\n",
            "Loss at Epoch 4900 is 0.014296\n",
            "final predictions:  [[0.02609514]\n",
            " [0.99419429]\n",
            " [0.98321414]\n",
            " [0.00650723]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F9vdgn7dhDXp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}